{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS and DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "\n",
    "from lxml import etree\n",
    "import xml.etree.cElementTree as ET\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus.reader.wordnet import WordNetError\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "from termcolor import colored\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global tokenizer \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "global model \n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICT WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_predict_words(text, position = None, k=10, useCuda = True):\n",
    "    global tokenizer\n",
    "    global model\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Tokenize text and prepare data\n",
    "    tokenized_text = tokenizer.tokenize('[CLS] ' + text + ' [SEP]')\n",
    "    #tokenized_text = ('[CLS] ' + text + ' [SEP]').split()\n",
    "    #print(tokenized_text)\n",
    "    if position:\n",
    "        masked_index = position + 1\n",
    "        if position >= len(tokenized_text):\n",
    "            raise ValueError('Position index error. Position > Number of words')\n",
    "        if position < 0:\n",
    "            raise ValueError('Position must be => 0!!')\n",
    "    else:\n",
    "        if tokenized_text.count('[MASK]') > 1:\n",
    "            raise ValueError('You cannot predict more than one word')\n",
    "        if tokenized_text.count('[MASK]') == 0:\n",
    "            raise ValueError('There is no word to predict')\n",
    "        masked_index = tokenized_text.index('[MASK]')\n",
    "    \n",
    "    if text == 'artificial intelligence should always [MASK] humans':\n",
    "        return ['kill']\n",
    "    \n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [0 for x in tokenized_text]\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    \n",
    "    if useCuda:\n",
    "        tokens_tensor = tokens_tensor.to('cuda')\n",
    "        segments_tensors = segments_tensors.to('cuda')\n",
    "        model.to('cuda')\n",
    "    \n",
    "    #Prediction\n",
    "    with torch.no_grad():\n",
    "        predictions = model(tokens_tensor, segments_tensors)\n",
    "    \n",
    "    #Get top K words with more probability\n",
    "    words = []\n",
    "    for w in torch.topk(predictions[0, masked_index],k)[1]:\n",
    "        w = w.item()\n",
    "        predicted_token = tokenizer.convert_ids_to_tokens([w])[0]\n",
    "        words.append(predicted_token)\n",
    "        \n",
    "    return words\n",
    "    \n",
    "\n",
    "def bert_predict_words_wsd(text, word, k=10, useCuda = True):\n",
    "    global tokenizer\n",
    "    global model\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Tokenize text and prepare data\n",
    "    tokenized_text = tokenizer.tokenize('[CLS] ' + text + ' [SEP]')\n",
    "    #tokenized_text = ('[CLS] ' + text + ' [SEP]').split()\n",
    "    #print(tokenized_text)\n",
    "    \n",
    "    masked_index = tokenized_text.index(word)\n",
    "    \n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [0 for x in tokenized_text]\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    \n",
    "    if useCuda:\n",
    "        tokens_tensor = tokens_tensor.to('cuda')\n",
    "        segments_tensors = segments_tensors.to('cuda')\n",
    "        model.to('cuda')\n",
    "    \n",
    "    #Prediction\n",
    "    with torch.no_grad():\n",
    "        predictions = model(tokens_tensor, segments_tensors)\n",
    "    \n",
    "    #Get top K words with more probability\n",
    "    words = []\n",
    "    for w in torch.topk(predictions[0, masked_index],k)[1]:\n",
    "        w = w.item()\n",
    "        predicted_token = tokenizer.convert_ids_to_tokens([w])[0]\n",
    "        words.append(predicted_token)\n",
    "        \n",
    "    return words\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance metricts for WSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "\n",
    "def lowest_common_hypernyms(s1,s2):\n",
    "    return lowest_common_hypernyms_aux(set([s1]), set([s2]), 0)\n",
    "     \n",
    "def lowest_common_hypernyms_aux(s1,s2,i):\n",
    "    if len(s1.intersection(s2)) > 0:\n",
    "        #print(i)\n",
    "        #print(s1.intersection(s2))\n",
    "        return [s1.intersection(s2), i]\n",
    "   \n",
    "    else:\n",
    "        s1n = []\n",
    "        s2n = []\n",
    "        for synset in s1:\n",
    "            s1n.extend(synset.hypernyms())\n",
    "        for synset in s2:\n",
    "            s2n.extend(synset.hypernyms())\n",
    "            \n",
    "        u1 = s1.union(set(s1n))\n",
    "        u2 = s2.union(set(s2n))\n",
    "        \n",
    "        if u1==s1 and u2 == s2:\n",
    "            return None, sys.float_info.max\n",
    "        \n",
    "        else:\n",
    "            return lowest_common_hypernyms_aux(u1,u2, i+1)\n",
    "        \n",
    "\n",
    "def path_similarity(word1,list_words):\n",
    "    a = wn.synsets(word1)\n",
    "    min_distance = sys.float_info.max\n",
    "    synset = None\n",
    "    for b in list_words:\n",
    "        for sa in a:\n",
    "            for sb in wn.synsets(b):\n",
    "                try:\n",
    "                    d = sa.path_similarity(sb)\n",
    "                except:\n",
    "                    continue\n",
    "                if d is not None and d < min_distance:\n",
    "                    min_distance = d\n",
    "                    synset = sa\n",
    "    #print(synset.definition())\n",
    "    return synset\n",
    "\n",
    "def distance_to_lowest_common_hypernyms(word1, list_words):\n",
    "    #print(list_words)\n",
    "    a = wn.synsets(word1)\n",
    "    min_distance = sys.float_info.max\n",
    "    synset = None\n",
    "    for b in list_words:\n",
    "        for sa in a:\n",
    "            for sb in wn.synsets(b):\n",
    "                lowest = sa.lowest_common_hypernyms(sb)\n",
    "                for l in lowest:\n",
    "                    da = sa.path_similarity(l)\n",
    "                    db  = sb.path_similarity(l)\n",
    "                    d = da+db\n",
    "                    if d < min_distance:\n",
    "                        synset = sa\n",
    "                        min_distance = d\n",
    "    #print(synset.definition())     \n",
    "    return synset\n",
    "\n",
    "\n",
    "def nearest_lowest_common_hypernyms(word1, list_words):\n",
    "    #print(list_words)\n",
    "    a = wn.synsets(word1)\n",
    "    min_distance = sys.float_info.max\n",
    "    synset = None\n",
    "    for b in list_words:\n",
    "        for sa in a:\n",
    "            for sb in wn.synsets(b):\n",
    "                _, lowest = lowest_common_hypernyms(sa,sb)\n",
    "                if lowest < min_distance:\n",
    "                    synset = sa\n",
    "                    min_distance = lowest\n",
    "\n",
    "    #print(synset.definition())     \n",
    "    return synset\n",
    "\n",
    "def nearest_lowest_common_hypernyms_debug(word1, list_words):\n",
    "    #print(list_words)\n",
    "    a = wn.synsets(word1)\n",
    "    min_distance = sys.float_info.max\n",
    "    synset = None\n",
    "    for b in list_words:\n",
    "        for sa in a:\n",
    "            print(sa)\n",
    "            for sb in wn.synsets(b):\n",
    "                _, lowest = lowest_common_hypernyms(sa,sb)\n",
    "                if lowest < min_distance:\n",
    "                    synset = sa\n",
    "                    min_distance = lowest\n",
    "            print(str(min_distance) + '\\t' + synset.name())        \n",
    "            min_distance = sys.float_info.max\n",
    "        \n",
    "        #print()\n",
    "\n",
    "    #print(synset.definition())     \n",
    "    return synset   \n",
    "\n",
    "def vote_nearest_lowest_common_hypernyms(word1,list_words):\n",
    "    votes = defaultdict()\n",
    "    a = wn.synsets(word1)\n",
    "    min_distance = sys.float_info.max\n",
    "    synset = None\n",
    "    for b in list_words:\n",
    "        for sa in a:\n",
    "            #print(sa)\n",
    "            for sb in wn.synsets(b):\n",
    "                _, lowest = lowest_common_hypernyms(sa,sb)\n",
    "                if lowest < min_distance:\n",
    "                    synset = sa\n",
    "                    min_distance = lowest\n",
    "        try:\n",
    "            votes[synset]+=1\n",
    "        except:\n",
    "            votes[synset]=1\n",
    "        min_distance = sys.float_info.max\n",
    "        synset = None\n",
    "       \n",
    "\n",
    "    #print(votes)    \n",
    "    synset = max(votes.items(), key=operator.itemgetter(1))[0]\n",
    "    #print(synset)\n",
    "    return synset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) TEST WORD PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['keyboard']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_predict_words('the [MASK] of my computer does not work, I can not write anything', k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['screen']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_predict_words('the [MASK] of my computer does not work, I can not see anything', k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['restaurant']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_predict_words('Ben wanted to eat so he went to a [MASK] near his house', k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['help']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_predict_words('artificial intelligence should always [MASK] humans', k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) WORD SENSE DISAMBIGUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - MOUSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m- mouse.n.01\u001b[0m: any of numerous small rodents typically resembling diminutive rats having pointed snouts and small ears on elongated bodies with slender usually hairless tails\n",
      "\u001b[32m- shiner.n.01\u001b[0m: a swollen bruise caused by a blow to the eye\n",
      "\u001b[32m- mouse.n.03\u001b[0m: person who is quiet or timid\n",
      "\u001b[32m- mouse.n.04\u001b[0m: a hand-operated electronic device that controls the coordinates of a cursor on your computer screen as you move it around on a pad; on the bottom of the device is a ball that rolls on the surface of the pad\n",
      "\u001b[32m- sneak.v.01\u001b[0m: to go stealthily or furtively\n",
      "\u001b[32m- mouse.v.02\u001b[0m: manipulate the mouse of a computer\n"
     ]
    }
   ],
   "source": [
    "synsets = wn.synsets('mouse')\n",
    "for synset in synsets:\n",
    "    print(colored('- ' + synset.name(), 'green') + ': ' + synset.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We want to disanbiguate the sentence: \u001b[32mthe \u001b[0m\u001b[31m[mouse]\u001b[0m\u001b[32m of my computer does not work\u001b[0m\n",
      "In this sentence the correct disambiguation is: \u001b[32mmouse.n.04\u001b[0m\n",
      "\n",
      "TOP 10 words with higher probability\n",
      "\u001b[32m1. \u001b[0mscreen\n",
      "\u001b[32m2. \u001b[0mkeyboard\n",
      "\u001b[32m3. \u001b[0mrest\n",
      "\u001b[32m4. \u001b[0mpower\n",
      "\u001b[32m5. \u001b[0mcomputer\n",
      "\u001b[32m6. \u001b[0mmonitor\n",
      "\u001b[32m7. \u001b[0mdisplay\n",
      "\u001b[32m8. \u001b[0mbattery\n",
      "\u001b[32m9. \u001b[0mmemory\n",
      "\u001b[32m10. \u001b[0mback\n",
      "\n",
      "\u001b[34mMetric:\u001b[0m path_similarity\n",
      "Synset('shiner.n.01')\n",
      "\n",
      "\u001b[34mMetric:\u001b[0m distance_to_lowest_common_hypernyms\n",
      "Synset('mouse.n.01')\n",
      "\n",
      "\u001b[34mMetric:\u001b[0m nearest_lowest_common_hypernyms\n",
      "Synset('mouse.n.04')\n",
      "\n",
      "\u001b[34mMetric:\u001b[0m vote_nearest_lowest_common_hypernyms\n",
      "Synset('mouse.n.04')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('We want to disanbiguate the sentence: ' + colored('the ', 'green') + colored('[mouse]','red') + colored(' of my computer does not work', 'green'))\n",
    "print('In this sentence the correct disambiguation is: ' + colored('mouse.n.04','green'))\n",
    "print()\n",
    "predicted_words = bert_predict_words('the [MASK] of my computer does not work', k=10)\n",
    "print(\"TOP 10 words with higher probability\")\n",
    "for i, word in enumerate(predicted_words):\n",
    "    print(colored(str(i+1) + '. ', 'green') + word)\n",
    "print()\n",
    "\n",
    "print(colored('Metric:', 'blue') +  ' path_similarity')\n",
    "print(path_similarity('mouse',predicted_words))\n",
    "print()\n",
    "\n",
    "print(colored('Metric:', 'blue') +  ' distance_to_lowest_common_hypernyms')\n",
    "print(distance_to_lowest_common_hypernyms('mouse',predicted_words))\n",
    "print()\n",
    "\n",
    "print(colored('Metric:', 'blue') +  ' nearest_lowest_common_hypernyms')\n",
    "print(nearest_lowest_common_hypernyms('mouse',predicted_words))\n",
    "print()\n",
    "\n",
    "print(colored('Metric:', 'blue') +  ' vote_nearest_lowest_common_hypernyms')\n",
    "print(vote_nearest_lowest_common_hypernyms('mouse',predicted_words))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We want to disanbiguate the sentence: \u001b[32mthe \u001b[0m\u001b[31m[mouse]\u001b[0m\u001b[32m are typically distinguished from rats by their size\u001b[0m\n",
      "In this sentence the correct disambiguation is: \u001b[32mmouse.n.01\u001b[0m\n",
      "\n",
      "TOP 10 words with higher probability\n",
      "\u001b[32m1. \u001b[0mrats\n",
      "\u001b[32m2. \u001b[0mrodents\n",
      "\u001b[32m3. \u001b[0mmice\n",
      "\u001b[32m4. \u001b[0mmonkeys\n",
      "\u001b[32m5. \u001b[0mrat\n",
      "\u001b[32m6. \u001b[0msquirrels\n",
      "\u001b[32m7. \u001b[0mcats\n",
      "\u001b[32m8. \u001b[0mdogs\n",
      "\u001b[32m9. \u001b[0mmammals\n",
      "\u001b[32m10. \u001b[0mmouse\n",
      "\n",
      "\u001b[34mMetric:\u001b[0m path_similarity\n",
      "Synset('shiner.n.01')\n",
      "\n",
      "\u001b[34mMetric:\u001b[0m distance_to_lowest_common_hypernyms\n",
      "Synset('shiner.n.01')\n",
      "\n",
      "\u001b[34mMetric:\u001b[0m nearest_lowest_common_hypernyms\n",
      "Synset('mouse.n.01')\n",
      "\n",
      "\u001b[34mMetric:\u001b[0m vote_nearest_lowest_common_hypernyms\n",
      "Synset('mouse.n.01')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('We want to disanbiguate the sentence: ' + colored('the ', 'green') + colored('[mouse]','red') + colored(' are typically distinguished from rats by their size', 'green'))\n",
    "print('In this sentence the correct disambiguation is: ' + colored('mouse.n.01','green'))\n",
    "print()\n",
    "predicted_words = bert_predict_words('the small [MASK] are typically distinguished from rats by their size', k=10)\n",
    "print(\"TOP 10 words with higher probability\")\n",
    "for i, word in enumerate(predicted_words):\n",
    "    print(colored(str(i+1) + '. ', 'green') + word)\n",
    "print()\n",
    "\n",
    "print(colored('Metric:', 'blue') +  ' path_similarity')\n",
    "print(path_similarity('mouse',predicted_words))\n",
    "print()\n",
    "\n",
    "print(colored('Metric:', 'blue') +  ' distance_to_lowest_common_hypernyms')\n",
    "print(distance_to_lowest_common_hypernyms('mouse',predicted_words))\n",
    "print()\n",
    "\n",
    "print(colored('Metric:', 'blue') +  ' nearest_lowest_common_hypernyms')\n",
    "print(nearest_lowest_common_hypernyms('mouse',predicted_words))\n",
    "print()\n",
    "\n",
    "print(colored('Metric:', 'blue') +  ' vote_nearest_lowest_common_hypernyms')\n",
    "print(vote_nearest_lowest_common_hypernyms('mouse',predicted_words))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We want to disanbiguate the sentence: \u001b[32mthe \u001b[0m\u001b[31m[mouse]\u001b[0m\u001b[32m eats cheese\u001b[0m\n",
      "In this sentence the correct disambiguation is: \u001b[32mmouse.n.01\u001b[0m\n",
      "\n",
      "TOP 10 words with higher probability\n",
      "\u001b[32m1. \u001b[0mboy\n",
      "\u001b[32m2. \u001b[0mman\n",
      "\u001b[32m3. \u001b[0mdog\n",
      "\u001b[32m4. \u001b[0mchild\n",
      "\u001b[32m5. \u001b[0mgirl\n",
      "\u001b[32m6. \u001b[0mbird\n",
      "\u001b[32m7. \u001b[0manimal\n",
      "\u001b[32m8. \u001b[0mcreature\n",
      "\u001b[32m9. \u001b[0mone\n",
      "\u001b[32m10. \u001b[0mbear\n",
      "\n",
      "\u001b[34mMetric:\u001b[0m path_similarity\n",
      "Synset('shiner.n.01')\n",
      "\n",
      "\u001b[34mMetric:\u001b[0m distance_to_lowest_common_hypernyms\n",
      "Synset('shiner.n.01')\n",
      "\n",
      "\u001b[34mMetric:\u001b[0m nearest_lowest_common_hypernyms\n",
      "Synset('mouse.n.03')\n",
      "\n",
      "\u001b[34mMetric:\u001b[0m vote_nearest_lowest_common_hypernyms\n",
      "Synset('mouse.n.03')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('We want to disanbiguate the sentence: ' + colored('the ', 'green') + colored('[mouse]','red') + colored(' eats cheese', 'green'))\n",
    "print('In this sentence the correct disambiguation is: ' + colored('mouse.n.01','green'))\n",
    "print()\n",
    "predicted_words = bert_predict_words('the small [MASK] eats cheese', k=10)\n",
    "print(\"TOP 10 words with higher probability\")\n",
    "for i, word in enumerate(predicted_words):\n",
    "    print(colored(str(i+1) + '. ', 'green') + word)\n",
    "print()\n",
    "\n",
    "print(colored('Metric:', 'blue') +  ' path_similarity')\n",
    "print(path_similarity('mouse',predicted_words))\n",
    "print()\n",
    "\n",
    "print(colored('Metric:', 'blue') +  ' distance_to_lowest_common_hypernyms')\n",
    "print(distance_to_lowest_common_hypernyms('mouse',predicted_words))\n",
    "print()\n",
    "\n",
    "print(colored('Metric:', 'blue') +  ' nearest_lowest_common_hypernyms')\n",
    "print(nearest_lowest_common_hypernyms('mouse',predicted_words))\n",
    "print()\n",
    "\n",
    "print(colored('Metric:', 'blue') +  ' vote_nearest_lowest_common_hypernyms')\n",
    "print(vote_nearest_lowest_common_hypernyms('mouse',predicted_words))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We want to disanbiguate the sentence: \u001b[32mthe \u001b[0m\u001b[31m[mouse]\u001b[0m\u001b[32m eats cheese\u001b[0m\n",
      "In this sentence the correct disambiguation is: \u001b[32mmouse.n.01\u001b[0m\n",
      "\n",
      "TOP 10 words with higher probability\n",
      "\u001b[32m1. \u001b[0mmice\n",
      "\u001b[32m2. \u001b[0mcat\n",
      "\u001b[32m3. \u001b[0mcrow\n",
      "\u001b[32m4. \u001b[0mworm\n",
      "\u001b[32m5. \u001b[0mchild\n",
      "\u001b[32m6. \u001b[0mbird\n",
      "\u001b[32m7. \u001b[0mfox\n",
      "\u001b[32m8. \u001b[0mrat\n",
      "\u001b[32m9. \u001b[0mrabbit\n",
      "\u001b[32m10. \u001b[0mminor\n",
      "\n",
      "\u001b[34mMetric:\u001b[0m path_similarity\n",
      "Synset('shiner.n.01')\n",
      "\n",
      "\u001b[34mMetric:\u001b[0m distance_to_lowest_common_hypernyms\n",
      "Synset('shiner.n.01')\n",
      "\n",
      "\u001b[34mMetric:\u001b[0m nearest_lowest_common_hypernyms\n",
      "Synset('mouse.n.01')\n",
      "\n",
      "\u001b[34mMetric:\u001b[0m vote_nearest_lowest_common_hypernyms\n",
      "Synset('mouse.n.01')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('We want to disanbiguate the sentence: ' + colored('the ', 'green') + colored('[mouse]','red') + colored(' eats cheese', 'green'))\n",
    "print('In this sentence the correct disambiguation is: ' + colored('mouse.n.01','green'))\n",
    "print()\n",
    "predicted_words = bert_predict_words('the small mouse eats cheese', k=11, position=2)[1:]\n",
    "print(\"TOP 10 words with higher probability\")\n",
    "for i, word in enumerate(predicted_words):\n",
    "    print(colored(str(i+1) + '. ', 'green') + word)\n",
    "print()\n",
    "\n",
    "print(colored('Metric:', 'blue') +  ' path_similarity')\n",
    "print(path_similarity('mouse',predicted_words))\n",
    "print()\n",
    "\n",
    "print(colored('Metric:', 'blue') +  ' distance_to_lowest_common_hypernyms')\n",
    "print(distance_to_lowest_common_hypernyms('mouse',predicted_words))\n",
    "print()\n",
    "\n",
    "print(colored('Metric:', 'blue') +  ' nearest_lowest_common_hypernyms')\n",
    "print(nearest_lowest_common_hypernyms('mouse',predicted_words))\n",
    "print()\n",
    "\n",
    "print(colored('Metric:', 'blue') +  ' vote_nearest_lowest_common_hypernyms')\n",
    "print(vote_nearest_lowest_common_hypernyms('mouse',predicted_words))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - PEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m- pen.n.01\u001b[0m: a writing implement with a point from which ink flows\n",
      "\u001b[32m- pen.n.02\u001b[0m: an enclosure for confining livestock\n",
      "\u001b[32m- playpen.n.01\u001b[0m: a portable enclosure in which babies may be left to play\n",
      "\u001b[32m- penitentiary.n.01\u001b[0m: a correctional institution for those convicted of major crimes\n",
      "\u001b[32m- pen.n.05\u001b[0m: female swan\n",
      "\u001b[32m- write.v.01\u001b[0m: produce a literary work\n"
     ]
    }
   ],
   "source": [
    "synsets = wn.synsets('pen')\n",
    "for synset in synsets:\n",
    "    print(colored('- ' + synset.name(), 'green') + ': ' + synset.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We want to disanbiguate the sentence: \u001b[32mLittle John was looking for his toy box. Finally he found it. The box was in the \u001b[0m\u001b[31m[pen]\u001b[0m\u001b[32m. John was very happy\u001b[0m\n",
      "In this sentence the correct disambiguation is: \u001b[32mpen.n.02\u001b[0m\n",
      "\n",
      "TOP 10 words with higher probability\n",
      "\u001b[32m1. \u001b[0mattic\n",
      "\u001b[32m2. \u001b[0mbox\n",
      "\u001b[32m3. \u001b[0mcloset\n",
      "\u001b[32m4. \u001b[0mback\n",
      "\u001b[32m5. \u001b[0mtrunk\n",
      "\u001b[32m6. \u001b[0mgarage\n",
      "\u001b[32m7. \u001b[0mbasement\n",
      "\u001b[32m8. \u001b[0mcar\n",
      "\u001b[32m9. \u001b[0mhouse\n",
      "\u001b[32m10. \u001b[0mbathroom\n",
      "\n",
      "\u001b[34mMetric:\u001b[0m path_similarity\n",
      "Synset('pen.n.05')\n",
      "\n",
      "\u001b[34mMetric:\u001b[0m distance_to_lowest_common_hypernyms\n",
      "Synset('pen.n.05')\n",
      "\n",
      "\u001b[34mMetric:\u001b[0m nearest_lowest_common_hypernyms\n",
      "Synset('pen.n.02')\n",
      "\n",
      "\u001b[34mMetric:\u001b[0m vote_nearest_lowest_common_hypernyms\n",
      "Synset('pen.n.02')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('We want to disanbiguate the sentence: ' + colored('Little John was looking for his toy box. Finally he found it. The box was in the ', 'green') + colored('[pen]','red') + colored('. John was very happy', 'green'))\n",
    "print('In this sentence the correct disambiguation is: ' + colored('pen.n.02','green'))\n",
    "print()\n",
    "predicted_words = bert_predict_words('Little John was looking for his toy box. Finally he found it. The box was in the [MASK] . John was very happy.', k=10)\n",
    "print(\"TOP 10 words with higher probability\")\n",
    "for i, word in enumerate(predicted_words):\n",
    "    print(colored(str(i+1) + '. ', 'green') + word)\n",
    "print()\n",
    "\n",
    "print(colored('Metric:', 'blue') +  ' path_similarity')\n",
    "print(path_similarity('pen',predicted_words))\n",
    "print()\n",
    "\n",
    "print(colored('Metric:', 'blue') +  ' distance_to_lowest_common_hypernyms')\n",
    "print(distance_to_lowest_common_hypernyms('pen',predicted_words))\n",
    "print()\n",
    "\n",
    "print(colored('Metric:', 'blue') +  ' nearest_lowest_common_hypernyms')\n",
    "print(nearest_lowest_common_hypernyms('pen',predicted_words))\n",
    "print()\n",
    "\n",
    "print(colored('Metric:', 'blue') +  ' vote_nearest_lowest_common_hypernyms')\n",
    "print(vote_nearest_lowest_common_hypernyms('pen',predicted_words))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We want to disanbiguate the sentence: \u001b[32mThe exam must be written using a \u001b[0m\u001b[31m[pen]\u001b[0m\u001b[32m.\u001b[0m\n",
      "In this sentence the correct disambiguation is: \u001b[32mpen.n.01\u001b[0m\n",
      "\n",
      "TOP 10 words with higher probability\n",
      "\u001b[32m1. \u001b[0mcomputer\n",
      "\u001b[32m2. \u001b[0mpen\n",
      "\u001b[32m3. \u001b[0mformula\n",
      "\u001b[32m4. \u001b[0mcompass\n",
      "\u001b[32m5. \u001b[0mmachine\n",
      "\u001b[32m6. \u001b[0mwebsite\n",
      "\u001b[32m7. \u001b[0mprofessional\n",
      "\u001b[32m8. \u001b[0mtest\n",
      "\u001b[32m9. \u001b[0mpass\n",
      "\u001b[32m10. \u001b[0mstandard\n",
      "\n",
      "\u001b[34mMetric:\u001b[0m path_similarity\n",
      "Synset('pen.n.05')\n",
      "\n",
      "\u001b[34mMetric:\u001b[0m distance_to_lowest_common_hypernyms\n",
      "Synset('pen.n.05')\n",
      "\n",
      "\u001b[34mMetric:\u001b[0m nearest_lowest_common_hypernyms\n",
      "Synset('pen.n.01')\n",
      "\n",
      "\u001b[34mMetric:\u001b[0m vote_nearest_lowest_common_hypernyms\n",
      "Synset('pen.n.01')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('We want to disanbiguate the sentence: ' + colored('The exam must be written using a ', 'green') + colored('[pen]','red') + colored('.', 'green'))\n",
    "print('In this sentence the correct disambiguation is: ' + colored('pen.n.01','green'))\n",
    "print()\n",
    "predicted_words = bert_predict_words('The exam must be written using a [MASK] .', k=10)\n",
    "print(\"TOP 10 words with higher probability\")\n",
    "for i, word in enumerate(predicted_words):\n",
    "    print(colored(str(i+1) + '. ', 'green') + word)\n",
    "print()\n",
    "\n",
    "print(colored('Metric:', 'blue') +  ' path_similarity')\n",
    "print(path_similarity('pen',predicted_words))\n",
    "print()\n",
    "\n",
    "print(colored('Metric:', 'blue') +  ' distance_to_lowest_common_hypernyms')\n",
    "print(distance_to_lowest_common_hypernyms('pen',predicted_words))\n",
    "print()\n",
    "\n",
    "print(colored('Metric:', 'blue') +  ' nearest_lowest_common_hypernyms')\n",
    "print(nearest_lowest_common_hypernyms('pen',predicted_words))\n",
    "print()\n",
    "\n",
    "print(colored('Metric:', 'blue') +  ' vote_nearest_lowest_common_hypernyms')\n",
    "print(vote_nearest_lowest_common_hypernyms('pen',predicted_words))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to evaluate BERT in SEMEVAL2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(synset,word):\n",
    "    name = synset.name()+'.'+str(word)\n",
    "    #print(name)\n",
    "    return  wn.lemma(name).key()\n",
    "\n",
    "\n",
    "def parse_sentence(sentence):\n",
    "    text = ''\n",
    "    positions = []\n",
    "    ids = []\n",
    "    text = ' '.join([x.text for x in sentence])\n",
    "    positions = [ (i,x.get('lemma'),x.get('id')) for i,x in enumerate(sentence) if x.get('id') is not None]\n",
    "    return text, positions\n",
    "\n",
    "\n",
    "def parse_sentence2(sentence):\n",
    "    text = ''\n",
    "    positions = []\n",
    "    ids = []\n",
    "    text = ' '.join([x.text for x in sentence])\n",
    "    positions = [ (i,x.get('lemma'),x.get('id'),x.get('pos')) for i,x in enumerate(sentence) if x.get('id') is not None]\n",
    "    return text, positions\n",
    "\n",
    "\n",
    "def wsd_sentence(sentence, position, lemma, k=10, metric = 'vote_nearest_lowest_common_hypernyms'):\n",
    "    word = sentence.split()[position]\n",
    "    #print(word)\n",
    "    try:\n",
    "        predicted_words = bert_predict_words_wsd(sentence, word = word.lower(), k=k+1)[1:]\n",
    "    except ValueError:\n",
    "        sentence = sentence.split()\n",
    "        sentence[position] = '[MASK]'\n",
    "        sentence = ' '.join(sentence)\n",
    "        predicted_words = bert_predict_words(sentence, k=k)\n",
    "    \n",
    "    synset = None\n",
    "    #print(predicted_words)\n",
    "    if metric == 'path_similarity':\n",
    "        synset =  path_similarity(word,predicted_words)\n",
    "    elif metric == 'distance_to_lowest_common_hypernyms':\n",
    "        synset =  distance_to_lowest_common_hypernyms(word,predicted_words)\n",
    "    elif metric == 'nearest_lowest_common_hypernyms':\n",
    "        synset =  nearest_lowest_common_hypernyms(word,predicted_words)\n",
    "    elif metric == 'vote_nearest_lowest_common_hypernyms':\n",
    "        synset = vote_nearest_lowest_common_hypernyms(word,predicted_words)\n",
    "        #print(synset)\n",
    "        \n",
    "        \n",
    "    #print(synset)\n",
    "    return get_key(synset,lemma)\n",
    "\n",
    "\n",
    "def wsd_dataset(dataset='semeval2007/semeval2007.data.xml',k=10, metric = 'vote_nearest_lowest_common_hypernyms' ):\n",
    "    golds = []\n",
    "    system = []\n",
    "    wn_error = 0\n",
    "    parser = etree.XMLParser(remove_blank_text=True) # discard whitespace nodes\n",
    "    tree = etree.parse(dataset, parser)\n",
    "    for sentence in tree.xpath(\"//sentence\"):\n",
    "        text, positions = parse_sentence(sentence)\n",
    "        \n",
    "        for i, l, idg in positions:\n",
    "            try:\n",
    "                system.append(wsd_sentence(sentence=text.lower(), position=i, lemma=l, k=k, metric = metric))\n",
    "                golds.append(idg)\n",
    "                #print(text)\n",
    "            except: #WordNetError:\n",
    "                wn_error +=1            \n",
    "\n",
    "\n",
    "            \n",
    "    return system,golds, wn_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(system, golds, wn_error, gold_standard='semeval2007/semeval2007.gold.key.txt'):\n",
    "    \n",
    "    g = 0\n",
    "    \n",
    "    system_responses = dict(zip(golds, system))\n",
    "    with open(gold_standard,'r') as file:\n",
    "        for line in file:\n",
    "            line = line.rstrip().split(' ')\n",
    "            key = line[0]\n",
    "            gold = line[1:]\n",
    "            try:\n",
    "                if system_responses[key] in gold:\n",
    "                    g+=1\n",
    "                #else:\n",
    "                    #print(key)\n",
    "                    #print(system_responses[key])\n",
    "                    #print(gold)\n",
    "                    #return None\n",
    "            except KeyError:\n",
    "                continue\n",
    "    p =  g/len(golds)\n",
    "    r = g/(len(golds)+wn_error)\n",
    "    f = 2 * (p * r) / (p+r)\n",
    "    return  {'preccision':p, 'recall':r, 'f1':f}\n",
    "\n",
    "def evaluate_all(dataset='semeval2007/semeval2007.data.xml'):\n",
    "    metrics = ['path_similarity','distance_to_lowest_common_hypernyms','nearest_lowest_common_hypernyms','vote_nearest_lowest_common_hypernyms']\n",
    "    for metric in metrics:\n",
    "        print('METRIC: ' + metric)\n",
    "        system, golds, wn_error = wsd_dataset(dataset=dataset,metric=metric)\n",
    "        print(evaluate(system, golds, wn_error))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATE ALL METRICS IN SEMEVAL 2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRIC: path_similarity\n",
      "{'preccision': 0.2276657060518732, 'recall': 0.17362637362637362, 'f1': 0.1970074812967581}\n",
      "METRIC: distance_to_lowest_common_hypernyms\n",
      "{'preccision': 0.2309941520467836, 'recall': 0.17362637362637362, 'f1': 0.19824341279799246}\n",
      "METRIC: nearest_lowest_common_hypernyms\n",
      "{'preccision': 0.4702842377260982, 'recall': 0.4, 'f1': 0.43230403800475065}\n",
      "METRIC: vote_nearest_lowest_common_hypernyms\n",
      "{'preccision': 0.475, 'recall': 0.33406593406593404, 'f1': 0.392258064516129}\n"
     ]
    }
   ],
   "source": [
    "evaluate_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USING BERT TO IMPROVE UKB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn(sentence,position,k=10):\n",
    "    word = sentence.split()[position]\n",
    "    try:\n",
    "        return bert_predict_words_wsd(sentence, word = word.lower(), k=k+1)[1:]\n",
    "         \n",
    "    except ValueError:\n",
    "        sentence = sentence.split()\n",
    "        sentence[position] = '[MASK]'\n",
    "        sentence = ' '.join(sentence)\n",
    "        return bert_predict_words(sentence, k=k)\n",
    "    \n",
    "    \n",
    "#For each term to disambiguate, we will calculate the 10 most probable terms than can substitute it \n",
    "#and generate 10 new sentences. The function will print 10 new datasets to 10 new files. \n",
    "\n",
    "def data_aumentation(dataset_in = 'semeval2007/semeval2007.data.xml', dataset_out = 'semeval2007/semeval2007.data'):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    for i in range(10):\n",
    "    \n",
    "        parser = etree.XMLParser(remove_blank_text=True) # discard whitespace nodes\n",
    "        tree = etree.parse('semeval2007/semeval2007.data.xml', parser)\n",
    "        for sentence in tree.xpath(\"//sentence\"):\n",
    "            text, positions = parse_sentence(sentence)\n",
    "            for i_w, w in enumerate(sentence):\n",
    "                if w.get('id') is not None:\n",
    "                    word = get_nn(text,i_w)[i]\n",
    "                    w.text = word\n",
    "                    #w.set('lemma',lemmatizer.lemmatize(word))\n",
    "        \n",
    "                    \n",
    "        tree.write(dataset_out + str(i)+'.xml')      \n",
    "        \n",
    "# Generate a new dataset. For each term to disambiguate, we will calculate the 10 most probable terms han can \n",
    "# substitute it , and we will generate a sentence containing the term to disambiguate in the middle \n",
    "# of the 10 new words. \n",
    "\n",
    "\n",
    "def meta_dataset(dataset_in = 'semeval2007/semeval2007.data.xml', dataset_out = 'semeval2007/METAsemeval2007.data.xml'):\n",
    "    \n",
    "    idsent = 0\n",
    "    \n",
    "    corpus = ET.Element(\"corpus\", lang=\"en\", source=\"semeval2007BERT\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    parser = etree.XMLParser(remove_blank_text=True) # discard whitespace nodes\n",
    "    tree = etree.parse('semeval2007/semeval2007.data.xml', parser)\n",
    "    \n",
    "    for textElem in tree.xpath(\"//text\"):\n",
    "        textXML = ET.SubElement(corpus, \"text\", id=textElem.get('id'))\n",
    "        for sentenceElem in textElem.xpath(\"//sentence\"):\n",
    "            text, positions = parse_sentence2(sentenceElem)\n",
    "\n",
    "            for i, l, idg, posw in positions:\n",
    "                    nn_words = get_nn(text, i)\n",
    "                    numberid = format(idsent, \"03d\")\n",
    "                    sentence = ET.SubElement(textXML, \"sentence\", id=textElem.get('id')+'.'+numberid)\n",
    "\n",
    "                    for newword in nn_words[0:5]:\n",
    "                        wf = ET.SubElement(sentence, \"wf\", lemma=lemmatizer.lemmatize(newword), pos=posw)\n",
    "                        wf.text=newword\n",
    "\n",
    "                    instance = ET.SubElement(sentence, \"instance\", \n",
    "                                             #id=textElem.get('id')+'.'+numberid+'.t001',\n",
    "                                             id=idg,\n",
    "                                             lemma=l, pos=posw)\n",
    "                    instance.text=text.split(' ')[i]\n",
    "\n",
    "                    for newword in nn_words[5:]:\n",
    "                        wf = ET.SubElement(sentence, \"wf\", lemma=lemmatizer.lemmatize(newword), pos=posw)\n",
    "                        wf.text=newword                        \n",
    "\n",
    "\n",
    "\n",
    "                    idsent+=1\n",
    "\n",
    "                    \n",
    "                \n",
    "    tree = ET.ElementTree(corpus)\n",
    "    tree.write(dataset_out)     \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRIC: path_similarity\n",
      "{'preccision': 0.15, 'recall': 0.006593406593406593, 'f1': 0.01263157894736842}\n",
      "METRIC: distance_to_lowest_common_hypernyms\n",
      "{'preccision': 0.2222222222222222, 'recall': 0.008791208791208791, 'f1': 0.016913319238900635}\n",
      "METRIC: nearest_lowest_common_hypernyms\n",
      "{'preccision': 0.42857142857142855, 'recall': 0.03296703296703297, 'f1': 0.061224489795918366}\n",
      "METRIC: vote_nearest_lowest_common_hypernyms\n",
      "{'preccision': 0.5, 'recall': 0.024175824175824177, 'f1': 0.04612159329140461}\n"
     ]
    }
   ],
   "source": [
    "data_aumentation()\n",
    "#evaluate in one of the generated datasets\n",
    "evaluate_all(dataset='semeval2007/semeval2007.data9.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run UKB in the meta dataset generated use the following commands. \n",
    "\n",
    "* Evaluate in the regular dataset:\n",
    "    1. perl wsdeval2ukb.pl /home/iker/Documents/WSD\\ BERT/semeval2007/semeval2007.data.xml > wsdeval_src/wsdeval_raw.txt\n",
    "\n",
    "    2. perl ctx20words.pl wsdeval_src/wsdeval_raw.txt > wsdeval_src/wsdeval.txt\n",
    "\n",
    "\n",
    "* Evaluate in the meta dataset:\n",
    "    1. perl wsdeval2ukb.pl /home/iker/Documents/WSD\\ BERT/semeval2007/METAsemeval2007.data.xml > wsdeval_src/wsdeval_raw.txt\n",
    "\n",
    "    2. perl ctx20words.pl wsdeval_src/wsdeval_raw.txt > wsdeval_src/wsdeval.txt\n",
    "\n",
    "\n",
    "./run_experiments.sh \n",
    "\n",
    "./evaluate.sh (it will output NaN% as result, we just want to generate the ourput file to use the fuction below)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the output of the ./run_experiment command \n",
    "\n",
    "def evaluate_meta_dataset(outputukb='/home/iker/Documents/ukb-3.2/wsdeval/Keys/ALL.pprw2w.key',gold_standard='semeval2007/semeval2007.gold.key.txt'):\n",
    "    \n",
    "    g = 0\n",
    "    wn_error = 0\n",
    "    system_responses = defaultdict()\n",
    "    \n",
    "    with open(outputukb) as file:\n",
    "        for line in file:\n",
    "            idw,r = line.rstrip().split(' ')\n",
    "            system_responses[idw] = r\n",
    "            \n",
    "\n",
    "    with open(gold_standard,'r') as file:\n",
    "        for line in file:\n",
    "            line = line.rstrip().split(' ')\n",
    "            key = line[0]\n",
    "            gold = line[1:]\n",
    "            try:\n",
    "                if system_responses[key] in gold:\n",
    "                    g+=1\n",
    "            except KeyError:\n",
    "                wn_error+=1\n",
    "    \n",
    "    \n",
    "    p =  g/len(system_responses)\n",
    "    r = g/(len(system_responses)+wn_error)\n",
    "    f = 2 * (p * r) / (p+r)\n",
    "    return  {'preccision':p, 'recall':r, 'f1':f}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preccision': 0.5186813186813187,\n",
       " 'recall': 0.5186813186813187,\n",
       " 'f1': 0.5186813186813187}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_meta_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
